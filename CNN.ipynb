{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "dataReadyForCNN = sio.loadmat(\"DataForCNN.mat\")\n",
    "\n",
    "xTest = dataReadyForCNN[\"xTest\"]\n",
    "xTrain = dataReadyForCNN[\"xTrain\"]\n",
    "\n",
    "yTrainCond = dataReadyForCNN[\"yTrain\"]\n",
    "yTestCond = dataReadyForCNN[\"yTest\"]\n",
    "#yTrainWord = dataReadyForCNN[\"yTrainWord\"]\n",
    "#yTestWord = dataReadyForCNN[\"yTestWord\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(648, 1, 51, 61, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTest.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "xTest_knn=xTest.reshape((xTest.shape[0],np.prod(xTest.shape[1:])))\n",
    "xTrain_knn=xTrain.reshape((xTrain.shape[0],np.prod(xTrain.shape[1:])))\n",
    "\n",
    "#sio.savemat(\"knnData.mat\",{\"xTest_knn\":xTest_knn, \"xTrain_knn\":xTrain_knn, \"yTrainCond_knn\":yTrainCond, \"yTestCond_knn\":yTestCond,\"yTrainWord_knn\":yTrainWord, \"yTestWord_knn\":yTestWord})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2592, 71553)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain_knn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "from theano import shared\n",
    "from lasagne.layers import InputLayer, DenseLayer\n",
    "\n",
    "from mlp import LogRegr, HiddenLayer, DropoutLayer\n",
    "from convnet3d import ConvLayer, NormLayer, PoolLayer, RectLayer\n",
    "from activations import relu, tanh, sigmoid, softplus\n",
    "\n",
    "# xTrain = np.random.rand(500, 1, 51, 61, 23).astype('float64')\n",
    "\n",
    "dtensor5 = T.TensorType('float64', (False,)*5)\n",
    "x = dtensor5('x') # the input data\n",
    "y = T.ivector()\n",
    "\n",
    "# input = (nImages, nChannel(nFeatureMaps), nDim1, nDim2, nDim3)\n",
    "\n",
    "# layer1 (500, 5, 47, 56, 22)\n",
    "# layer2 (500, 5, 10, 12, 5)\n",
    "# layer3 (500, 3, 9, 11, 4)\n",
    "# layer4 (500, 3, 5, 6, 2)\n",
    "\n",
    "kernel_shape = (5,6,2)\n",
    "fMRI_shape = (51, 61, 23)\n",
    "n_in_maps = 1 # channel\n",
    "n_out_maps = 5 # num of feature maps, aka the depth of the neurons\n",
    "batch_size = 500\n",
    "\n",
    "# 1st: Convolution Layer\n",
    "layer1_input = x\n",
    "layer1 = ConvLayer(layer1_input, 1, 5, (5, 6, 2), fMRI_shape, \n",
    "                       batch_size, tanh)\n",
    "\n",
    "# print layer1.output.eval({x:xTrain[:500]}).shape\n",
    "\n",
    "# 2nd: Pool layer\n",
    "poolShape = (5, 5, 5)\n",
    "layer2 = PoolLayer(layer1.output, poolShape)\n",
    "\n",
    "# print layer2.output.eval({x:xTrain}).shape\n",
    "\n",
    "# 3rd: Convolution Layer\n",
    "layer3 = ConvLayer(layer2.output, 5, 3, (2, 2, 2), (10, 12, 5), \n",
    "                       500, tanh)\n",
    "\n",
    "# print layer3.output.eval({x:xTrain[:500]}).shape\n",
    "\n",
    "# 4th: Pool layer\n",
    "layer4 = PoolLayer(layer3.output, (2, 2, 2))\n",
    "\n",
    "# print layer4.output.eval({x:xTrain[:500]}).shape\n",
    "\n",
    "# 5th: Dense layer\n",
    "layer5_input = T.flatten(layer4.output, outdim=2)\n",
    "layer5 = HiddenLayer(layer5_input, n_in=180, n_out=500, activation=tanh)\n",
    "\n",
    "# layer5.output.eval({x:xTrain[:500]}).shape\n",
    "\n",
    "# 6th: Logistic layer\n",
    "layer6 = LogRegr(layer5.output, 500, 12, tanh)\n",
    "\n",
    "cost = layer6.negative_log_likelihood(y)\n",
    "\n",
    "# # create a function to compute the mistakes that are made by the model\n",
    "# test_model = theano.function(\n",
    "#     [index],\n",
    "#     layer6.errors(y),\n",
    "#     givens={\n",
    "#         x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "#         y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# validate_model = theano.function(\n",
    "#     [index],\n",
    "#     layer6.errors(yCond),\n",
    "#     givens={\n",
    "#         x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "#         y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# create a list of all model parameters to be fit by gradient descent\n",
    "params = layer5.params + layer3.params + layer1.params + layer6.params\n",
    "\n",
    "# create a list of gradients for all model parameters\n",
    "grads = T.grad(cost, params)\n",
    "\n",
    "# train_model is a function that updates the model parameters by\n",
    "# SGD Since this model has many parameters, it would be tedious to\n",
    "# manually create an update rule for each model parameter. We thus\n",
    "# create the updates list by automatically looping over all\n",
    "# (params[i], grads[i]) pairs.\n",
    "learning_rate=0.1\n",
    "\n",
    "updates = [\n",
    "    (param_i, param_i - learning_rate * grad_i)\n",
    "    for param_i, grad_i in zip(params, grads)\n",
    "]\n",
    "\n",
    "# allocate symbolic variables for the data\n",
    "index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "train_model = theano.function(\n",
    "    [index],\n",
    "    cost,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: shared(xTrain)[index * batch_size: (index + 1) * batch_size],\n",
    "        y: shared(yTrainCond)[index * batch_size: (index + 1) * batch_size]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 51, 61, 23)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain[1:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_valid_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-2c1350278107>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# compute zero-one loss on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             validation_losses = [validate_model(i) for i\n\u001b[0;32m---> 42\u001b[0;31m                                  in range(n_valid_batches)]\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mthis_validation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_valid_batches' is not defined"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# TRAIN MODEL #\n",
    "###############\n",
    "import timeit\n",
    "\n",
    "print('... training')\n",
    "\n",
    "n_train_batches = 3\n",
    "n_epochs=200\n",
    "\n",
    "# early-stopping parameters\n",
    "patience = 10000  # look as this many examples regardless\n",
    "patience_increase = 2  # wait this much longer when a new best is\n",
    "                       # found\n",
    "improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                               # considered significant\n",
    "validation_frequency = min(n_train_batches, patience // 2)\n",
    "                              # go through this many\n",
    "                              # minibatche before checking the network\n",
    "                              # on the validation set; in this case we\n",
    "                              # check every epoch\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "best_iter = 0\n",
    "test_score = 0.\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "epoch = 0\n",
    "done_looping = False\n",
    "\n",
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    epoch = epoch + 1\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "\n",
    "        minibatch_avg_cost = train_model(minibatch_index)\n",
    "        # iteration number\n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "        if (iter + 1) % validation_frequency == 0:\n",
    "            # compute zero-one loss on validation set\n",
    "            validation_losses = [validate_model(i) for i\n",
    "                                 in range(n_valid_batches)]\n",
    "            this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "            print(\n",
    "                'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                (\n",
    "                    epoch,\n",
    "                    minibatch_index + 1,\n",
    "                    n_train_batches,\n",
    "                    this_validation_loss * 100.\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # if we got the best validation score until now\n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                #improve patience if loss improvement is good enough\n",
    "                if (\n",
    "                    this_validation_loss < best_validation_loss *\n",
    "                    improvement_threshold\n",
    "                ):\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                best_validation_loss = this_validation_loss\n",
    "                best_iter = iter\n",
    "\n",
    "                # test it on the test set\n",
    "                test_losses = [test_model(i) for i\n",
    "                               in range(n_test_batches)]\n",
    "                test_score = numpy.mean(test_losses)\n",
    "\n",
    "                print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                       'best model %f %%') %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       test_score * 100.))\n",
    "\n",
    "        if patience <= iter:\n",
    "            done_looping = True\n",
    "            break\n",
    "\n",
    "            \n",
    "# end_time = timeit.default_timer()\n",
    "# print(('Optimization complete. Best validation score of %f %% '\n",
    "#        'obtained at iteration %i, with test performance %f %%') %\n",
    "#       (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "# print(('The code for file ' +\n",
    "#        os.path.split(__file__)[1] +\n",
    "#        ' ran for %.2fm' % ((end_time - start_time) / 60.)), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
